# 泡姆棋 (Popucom Chess)
泡姆棋是鹰角网络的游戏泡姆泡姆（Popucom）的一种在 9×9 棋盘上进行的策略棋类游戏，结合了棋子放置、三消机制和区域涂色元素。

## 游戏目标
在双方的步数用尽后，通过消除棋子和涂色地板，占据更多自己颜色的地板区域。

## 游戏规则
### 1. 棋盘与棋子
棋盘大小为 9×9 格。
玩家分为黑方（红方）和白方（绿方）。黑方先行。
每个玩家初始有 25 步（即 25 颗棋子）可供使用。
每个棋盘格有多种状态：
存在黑子
存在白子
地板被涂成黑色
地板被涂成白色
地板未涂色（空地）

### 2. 落子规则
玩家轮流落子。
合法落子位置必须满足以下条件：
该位置在棋盘范围内。
该位置当前没有任何棋子。
该位置的地板颜色与当前玩家的颜色匹配（例如，黑子只能下在黑地或空地上），或者该地板是未涂色的空地。
落子时，棋子会放置在该位置上，但不会立即改变脚下地板的颜色。

### 3. 三消与涂色
当玩家落下一颗棋子后，游戏会检查是否连成了 3 颗或以上的同色棋子（包括刚刚落下的这颗棋子）。检查方向包括：
水平方向
垂直方向
两个对角线方向
如果满足三消条件：
消除： 连成一线的 n 颗同色棋子（n≥3）将被从棋盘上移除。如果同时满足多个三消条件（例如，一个子同时触发了横向和纵向三消），则所有满足条件的连子都会被消除。
涂色： 以刚刚落下的棋子为中心，沿着发生三消的所有行、列或斜线方向，将地板涂成当前玩家的颜色。
涂色会向外延伸，直到遇到对方的棋子为止（对方的棋子会阻碍涂色，无法改变对方的棋子下的地板颜色）。
涂色不会被已涂色的地板阻碍。
被消除棋子下方的地板也会被涂色。

### 4. 终局判断
游戏在以下两种情况之一发生时结束：
所有步数用尽： 当黑白双方的 25 步棋全部用尽后（此时最后一步棋一定是白方下的），游戏结束。
统计棋盘上黑方涂色地板的数量和白方涂色地板的数量。
涂色地板多的一方获胜。
如果双方涂色地板数量相同，则判为平局。
一方无法落子： 如果某一方在自己的回合开始时，发现棋盘上已经没有合法的落子位置，则该方立即判负。这通常意味着棋盘上大部分区域已被对方占据或填满。

## 项目结构
本项目由多个 Python 文件和多个 C++ 文件组成：
### popucom_nn_interface.py 和 popucom_nn_model.py
定义泡姆棋的神经网络模型架构（基于卷积神经网络和残差连接），包含策略头和价值头。（通过在残差块中插入全局注意力模块进行了长程关系的改进，同时在注意力分数引入了相对坐标偏置以感知相对坐标）
### popucom_puct.py
实现基于神经网络的 PUCT (Polynomial Upper Confidence Trees) 搜索算法，用于在自对弈和评估中选择最佳行动。
### self_play_worker.py
负责生成自对弈数据。它使用当前训练好的神经网络和 PUCT 搜索来玩游戏，并记录游戏过程中的状态、MCTS 策略和最终结果。（建议根据自己的机器修改线程数）
### train_model.py
负责神经网络的训练。它加载 self_play_worker.py 生成的自对弈数据，并使用这些数据来更新神经网络的权重。（建议根据自己的机器适当修改）
### run_pipeline.py
一个自动化脚本，用于循环执行自对弈数据生成和模型训练，实现强化学习的持续迭代过程。
### popucom_chess_gui.py
游玩的ui界面。
### game.h 和 game.cpp
定义泡姆棋的游戏规则、棋盘状态和落子逻辑。
### puct.h 和 puct.cpp
实现基于神经网络的 PUCT (Polynomial Upper Confidence Trees) 搜索算法，用于在自对弈和评估中选择最佳行动。
### popucom_core.dll
由 game.h/game.cpp/puct.h/puct.cpp 编译获得

## 强化学习流水线
本项目采用 AlphaGo Zero 风格的强化学习流水线，通过模型自对弈和训练的循环来不断提升 AI 的棋力：

### 自对弈 (self_play_worker.py):
使用当前版本的神经网络模型（model.pth）和 PUCT 搜索进行多局游戏。
在每一步棋中，MCTS 搜索会根据当前模型评估棋盘状态，并给出更强的策略分布。
游戏过程中的棋盘状态、MCTS 产生的策略分布以及最终的游戏结果被记录下来，作为训练数据。
数据保存到 self_play_data/ 目录。

### 模型训练 (train_model.py):
加载 self_play_data/ 目录中最新生成的自对弈数据。
使用这些数据训练一个新的神经网络模型。训练目标是让模型的策略输出尽可能接近 MCTS 产生的策略分布，同时让模型的价值输出尽可能接近实际的游戏结果。
训练完成后，新的模型权重会保存为 model.pth，覆盖旧模型。

### 迭代 (run_pipeline.py):
run_pipeline.py 脚本自动化了上述两个步骤。它会不断循环：
运行 self_play_worker.py 生成新的自对弈数据。
运行 train_model.py 使用新数据训练模型。
这个循环使得 AI 能够从自己的经验中学习，不断提升棋力，无需人类专家数据。

## 如何运行
确保您已安装 Python 3.x 和 PyTorch。
各种软件包缺什么下什么
对 game.h/game.cpp/puct.h/puct.cpp 编译获得 popucom_core.dll
生成初始数据并训练:
首次运行，由于没有 model.pth，self_play_worker.py 会使用随机权重模型。
运行训练流水线：
```
python run_pipeline.py
```
